from transformers import BertForSequenceClassification, BertTokenizerFast
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from typing import List, Tuple
import torch
import re

# ===== è·¯å¾„é…ç½®ï¼ˆä½ å¯ä»¥è§†éœ€è¦å¤–ç½®åˆ° config.pyï¼‰=====
bert_path = r"C:\Users\zhongzhengchao\Desktop\chat_ai_project\EmoGpt\checkpoint-3412"
gpt2_path = r"C:\Users\zhongzhengchao\Desktop\chat_ai_project\EmoGpt\gpt2-3"

# ===== åŠ è½½æƒ…ç»ªè¯†åˆ«æ¨¡å‹ =====
bert_tokenizer = BertTokenizerFast.from_pretrained(bert_path)
bert_model = BertForSequenceClassification.from_pretrained(bert_path)
bert_model.eval()

id2label = {
    0: "content",
    1: "fear",
    2: "lonely",
    3: "negative",
    4: "positive",
    5: "sentimental"
}

# ===== åŠ è½½ GPT2 æ¨¡å‹ =====
gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_path)
gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token
gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_path)

# âœ… ä¸»å‡½æ•°ï¼šæ¥å—ä¸€æ®µç”¨æˆ·æ–‡æœ¬ï¼Œè¿”å› 15 æ¡è¯­ä¹‰ç›¸å…³å›å¤
#def generate_general_replies(user_text: str = "No one messages me anymore..") -> list[str]:
def generate_general_replies(user_text: str) -> Tuple[List[str], str]:
    # ===== æƒ…ç»ªè¯†åˆ« =====
    with torch.no_grad():
        bert_inputs = bert_tokenizer(user_text, return_tensors="pt", truncation=True, padding=True)
        logits = bert_model(**bert_inputs).logits
        emotion_id = torch.argmax(logits, dim=1).item()
        emotion_label = id2label[emotion_id]

    print(f"ğŸ§  Emotion recognition resultsï¼š{emotion_label}")
    # ===== æ„é€  Prompt å¹¶ç”Ÿæˆå›å¤ =====
    prompt = f"User ({emotion_label}): {user_text}\nAI:"
    input_ids = gpt2_tokenizer.encode(prompt, return_tensors="pt")

    outputs = gpt2_model.generate(
        input_ids,
        max_length=150,
        do_sample=True,
        top_k=150,
        top_p=0.92,
        temperature=0.9,
        num_return_sequences=20,
        pad_token_id=gpt2_tokenizer.eos_token_id
    )

    # ===== æ¸…æ´—ä¸å»é‡å›å¤æ–‡æœ¬ =====
    seen = set()
    replies = []

    for output in outputs:
        decoded = gpt2_tokenizer.decode(output, skip_special_tokens=True)

        # å°è¯•ç”¨ â€œReply:â€ æå–ï¼ˆä½ åŸæ¥çš„æ–¹æ³•ï¼‰
        match = re.search(r"Reply:\s*(.*?)([\";}\\]|$)", decoded)
        if match:
            reply = match.group(1).strip()
        else:
            reply = decoded.split("AI:")[-1].strip()

        # æ¸…é™¤æ‚å­—ç¬¦
        reply = re.sub(r"[\n\r\"{}]", "", reply)

        # å»é‡ + éç©º
        if reply and reply not in seen and len(reply) > 3:
            seen.add(reply)
            replies.append(reply)

        if len(replies) >= 15:
            break

    return replies
